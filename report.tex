\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{subcaption}
\usepackage{float}

% Code listing style
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    breaklines=true,
    frame=single,
    captionpos=b
}

\title{Memory-Efficient Attention Mechanisms:\\Implementation and Analysis of FlashAttention}
\author{Shuang Ma, Pablo Rodriguez, Pei Yu Lin\\
Department of Computer Science\\
University of California, Davis}
\date{November 2025}

\begin{document}

\maketitle

\begin{abstract}
Transformer models have become the foundation of modern large language models (LLMs), but their self-attention mechanism suffers from quadratic memory complexity with respect to sequence length. This limitation becomes critical as context windows expand to tens of thousands of tokens. In this work, we attempt to implement and analyze FlashAttention, a memory-efficient exact attention algorithm that leverages tiling and online softmax computation to reduce memory usage from $O(L^2)$ to $O(L)$. We provide educational implementations of FlashAttention-1, FlashAttention-2, and a Triton GPU kernel, showing up to 94\% memory savings and 4.1x speedup on an NVIDIA RTX 3070 GPU while maintaining numerical equivalence to standard attention. Our benchmarks across varying sequence lengths (512 to 8K tokens) compare CPU vs GPU overhead, pure PyTorch vs fused kernels, and explore both the theoretical memory advantages and practical performance characteristics of the algorithm.
\end{abstract}

\section{Introduction}

\subsection{Motivation}
Large Language Models (LLMs) such as GPT-4, Claude, and LLaMA rely on the Transformer architecture \cite{vaswani2017attention}, which uses self-attention as its core mechanism. The attention operation allows each token to attend to all other tokens in the sequence, enabling the model to capture long-range dependencies. However, this comes at a significant computational cost: for a sequence of length $L$, the attention mechanism requires $O(L^2)$ memory to store the attention matrix.

This quadratic scaling poses several challenges:
\begin{itemize}
    \item \textbf{Memory Constraints:} A sequence of 32K tokens requires storing a 32K $\times$ 32K matrix (1 billion elements, 4GB for float32)
    \item \textbf{GPU Memory Limits:} Consumer GPUs have 8-24GB memory, limiting context lengths
    \item \textbf{Training Costs:} Larger memory footprints require more expensive hardware
    \item \textbf{Inference Latency:} Memory bandwidth becomes the bottleneck, not computation
\end{itemize}

\subsection{Our Contribution}
In this project, we:
\begin{enumerate}
    \item Attempt to implement the FlashAttention algorithm from the paper in PyTorch
    \item Provide both FlashAttention-1 and FlashAttention-2 variants
    \item Show memory savings of up to 95\% compared to standard attention
    \item Verify numerical correctness with comprehensive test suites
    \item Analyze the trade-offs between memory efficiency and computational overhead
    \item Include causal masking support for autoregressive models
\end{enumerate}

\section{Background}

\subsection{Standard Self-Attention}
The self-attention mechanism computes:
\begin{equation}
    \text{Output} = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right) V
\end{equation}
where $Q, K, V \in \mathbb{R}^{L \times d}$ are the query, key, and value matrices, $L$ is the sequence length, and $d$ is the head dimension.

The standard implementation follows three steps:
\begin{align}
    S &= QK^T / \sqrt{d} & \text{(Scores, } O(L^2d) \text{ FLOPs)} \\
    P &= \text{softmax}(S) & \text{(Attention weights, } O(L^2) \text{ memory)} \\
    O &= PV & \text{(Output, } O(L^2d) \text{ FLOPs)}
\end{align}

The critical issue is that both $S$ and $P$ are $L \times L$ matrices that must be stored in GPU memory.

\subsection{GPU Memory Hierarchy}
Understanding FlashAttention requires knowledge of GPU memory architecture:
\begin{itemize}
    \item \textbf{HBM (High Bandwidth Memory):} Large capacity (40-80GB), but relatively slow access (1.5-2 TB/s)
    \item \textbf{SRAM (On-chip Cache):} Small capacity (20-200KB per SM), but extremely fast (19 TB/s on A100)
\end{itemize}

Standard attention is \textit{memory-bound}: the GPU spends more time moving data between HBM and compute units than performing actual calculations. FlashAttention addresses this by keeping intermediate results in SRAM.

\subsection{The Online Softmax Algorithm}
The key mathematical insight enabling FlashAttention is the \textit{online softmax} algorithm \cite{milakov2018online}. Traditional softmax requires two passes:
\begin{enumerate}
    \item Compute $m = \max(x_i)$ for numerical stability
    \item Compute $\text{softmax}(x_i) = \exp(x_i - m) / \sum_j \exp(x_j - m)$
\end{enumerate}

The online algorithm computes this incrementally using recurrence relations:
\begin{align}
    m_i &= \max(m_{i-1}, x_i) \\
    d_i' &= d_{i-1}' \cdot \exp(m_{i-1} - m_i) + \exp(x_i - m_i)
\end{align}

The crucial insight is the \textbf{rescaling} when the maximum changes: previous partial results are multiplied by $\exp(m_{\text{old}} - m_{\text{new}})$ to maintain mathematical equivalence.

\section{Method}

\subsection{FlashAttention-1 Algorithm}
Our implementation follows the tiled algorithm from \cite{dao2022flashattention}. For each query row, we:

\begin{algorithm}[H]
\caption{FlashAttention-1 (Tiled)}
\begin{algorithmic}[1]
\STATE Initialize: $m \leftarrow -\infty$, $d \leftarrow 0$, $o \leftarrow \mathbf{0}$
\FOR{each block $B$ of K, V}
    \STATE $\mathbf{x} \leftarrow Q_{\text{row}} \cdot K_B^T / \sqrt{d}$ \COMMENT{Local scores}
    \STATE $m_{\text{local}} \leftarrow \max(\mathbf{x})$
    \STATE $m_{\text{old}} \leftarrow m$, $d_{\text{old}} \leftarrow d$
    \STATE $m \leftarrow \max(m, m_{\text{local}})$ \COMMENT{Update global max}
    \STATE $\mathbf{e} \leftarrow \exp(\mathbf{x} - m)$ \COMMENT{Stable exponentials}
    \STATE $d \leftarrow d_{\text{old}} \cdot \exp(m_{\text{old}} - m) + \sum \mathbf{e}$ \COMMENT{Rescale denominator}
    \STATE $o \leftarrow o \cdot \frac{d_{\text{old}}}{d} \cdot \exp(m_{\text{old}} - m) + \frac{\sum \mathbf{e} \odot V_B}{d}$ \COMMENT{Rescale output}
\ENDFOR
\RETURN $o$
\end{algorithmic}
\end{algorithm}

\subsection{FlashAttention-2 Improvements}
FlashAttention-2 \cite{dao2023flashattention2} introduces key optimizations:

\begin{algorithm}[H]
\caption{FlashAttention-2 (Optimized)}
\begin{algorithmic}[1]
\STATE Initialize: $m \leftarrow -\infty$, $d \leftarrow 0$, $o_{\text{unnorm}} \leftarrow \mathbf{0}$
\FOR{each block $B$ of K, V}
    \STATE $\mathbf{x} \leftarrow Q_{\text{row}} \cdot K_B^T / \sqrt{d}$
    \STATE $m_{\text{old}} \leftarrow m$
    \STATE $m \leftarrow \max(m, \max(\mathbf{x}))$
    \STATE $\mathbf{e} \leftarrow \exp(\mathbf{x} - m)$
    \STATE $d \leftarrow d \cdot \exp(m_{\text{old}} - m) + \sum \mathbf{e}$
    \STATE $o_{\text{unnorm}} \leftarrow o_{\text{unnorm}} \cdot \exp(m_{\text{old}} - m) + \sum \mathbf{e} \odot V_B$ \COMMENT{No division!}
\ENDFOR
\RETURN $o_{\text{unnorm}} / d$ \COMMENT{Single normalization}
\end{algorithmic}
\end{algorithm}

The key difference: FlashAttention-2 maintains an \textit{unnormalized} output accumulator and performs a single division at the end, reducing FLOPs in the inner loop.

\subsection{Implementation Details}

Our implementation includes:
\begin{itemize}
    \item \textbf{Vectorized operations:} Process all query rows in parallel
    \item \textbf{Configurable block size:} Tunable parameter for memory/speed trade-off
    \item \textbf{Causal masking:} Support for autoregressive attention
    \item \textbf{Multi-head attention:} Full support for batches and multiple heads
\end{itemize}

\section{Experiments}

\subsection{Experimental Setup}
\begin{itemize}
    \item \textbf{Hardware:} NVIDIA RTX 3070 (8GB VRAM), Intel CPU
    \item \textbf{Software:} PyTorch 2.9.1, CUDA 12.1, Python 3.10
    \item \textbf{Precision:} Float32 for numerical stability
    \item \textbf{Metrics:} Peak memory usage, execution time, numerical accuracy
\end{itemize}

\subsection{Sequence Length Scaling}
We test sequence lengths from 512 to 8192 tokens with batch size 1, 8 attention heads, and head dimension 64 on an NVIDIA RTX 3070 GPU (8GB VRAM).

\begin{table}[H]
\centering
\caption{GPU Memory and Time Comparison: Standard vs FlashAttention (RTX 3070)}
\label{tab:memory}
\begin{tabular}{@{}rrrrrrr@{}}
\toprule
Seq Len & Std Time (ms) & Flash Time (ms) & Std Mem (MB) & Flash Mem (MB) & Savings & Max Diff \\
\midrule
512 & 0.59 & 2.12 & 29.12 & 23.22 & 20.3\% & $4.2 \times 10^{-7}$ \\
1024 & 1.38 & 4.05 & 82.12 & 38.31 & 53.3\% & $3.6 \times 10^{-7}$ \\
2048 & 4.33 & 8.59 & 284.12 & 68.50 & 75.9\% & $5.4 \times 10^{-7}$ \\
4096 & 14.28 & 29.01 & 1072.12 & 128.88 & 88.0\% & $3.9 \times 10^{-7}$ \\
8192 & 58.72 & 105.54 & 4184.12 & 249.62 & 94.0\% & $4.1 \times 10^{-7}$ \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{plots/memory_scaling.png}
\caption{Memory usage scaling with sequence length. Standard attention grows quadratically ($O(L^2)$), while FlashAttention grows linearly ($O(L)$). The gap widens significantly as sequence length increases.}
\label{fig:memory_scaling}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{plots/memory_savings.png}
\caption{Memory savings percentage increases with sequence length, reaching over 95\% for sequences longer than 4K tokens.}
\label{fig:memory_savings}
\end{figure}

\subsection{Execution Time Analysis}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{plots/time_comparison.png}
\caption{Execution time comparison. In pure PyTorch, FlashAttention is slower due to Python loop overhead. Real speedup requires CUDA kernel fusion.}
\label{fig:time_comparison}
\end{figure}

\textbf{Important Note:} Our pure PyTorch implementation shows slower execution times for FlashAttention. This is expected because:
\begin{enumerate}
    \item Python loops have significant overhead
    \item We don't actually use GPU SRAM (that requires CUDA kernels)
    \item Real FlashAttention fuses all operations into a single kernel
\end{enumerate}

The memory savings, however, are real and demonstrate the algorithmic advantage.

\subsection{Numerical Accuracy}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{plots/numerical_accuracy.png}
\caption{Maximum absolute difference between FlashAttention and standard attention output. All differences are well below the $10^{-4}$ threshold, confirming numerical equivalence.}
\label{fig:accuracy}
\end{figure}

Our comprehensive test suite (23 tests) validates:
\begin{itemize}
    \item Output matches standard attention within $10^{-6}$ error
    \item No NaN or Inf values even with extreme inputs
    \item Correct handling of various batch sizes and head configurations
    \item Causal masking produces correct autoregressive behavior
\end{itemize}

\subsection{Block Size Analysis}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{plots/block_size_effect.png}
\caption{Effect of block size on performance. Larger blocks reduce loop iterations but increase memory usage. The optimal block size depends on the specific hardware and sequence length.}
\label{fig:block_size}
\end{figure}

\subsection{FlashAttention-1 vs FlashAttention-2}

\begin{table}[H]
\centering
\caption{Comparison of FlashAttention Variants (seq\_len=512, batch=2, heads=4)}
\label{tab:v1v2}
\begin{tabular}{@{}lrrr@{}}
\toprule
Metric & FlashAttention-1 & FlashAttention-2 & Improvement \\
\midrule
Execution Time (ms) & 7.02 & 5.35 & 1.31x faster \\
Max Diff vs Standard & $2.98 \times 10^{-7}$ & $2.68 \times 10^{-7}$ & Better accuracy \\
Inner Loop Divisions & Per block & Once at end & Fewer FLOPs \\
\bottomrule
\end{tabular}
\end{table}

FlashAttention-2's optimization of keeping unnormalized accumulators results in measurable speedup even in our Python implementation.

\subsection{Model Dimension Scaling}

We tested FlashAttention's effectiveness across different hidden dimensions (512 to 8192) with a fixed sequence length of 2048 tokens.

\begin{table}[H]
\centering
\caption{Memory Savings vs Hidden Dimension (seq\_len=2048, 8 heads)}
\label{tab:hidden}
\begin{tabular}{@{}rrrrrr@{}}
\toprule
Hidden Size & Std Mem (MB) & Flash Mem (MB) & Savings & Max Diff \\
\midrule
512 & 284.12 & 52.50 & 81.5\% & $4.9 \times 10^{-7}$ \\
1024 & 304.12 & 88.50 & 70.9\% & $8.9 \times 10^{-7}$ \\
2048 & 344.12 & 160.50 & 53.4\% & $5.4 \times 10^{-7}$ \\
4096 & 424.12 & 304.50 & 28.2\% & $5.4 \times 10^{-7}$ \\
8192 & 584.12 & 592.50 & -1.4\% & $4.6 \times 10^{-7}$ \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Finding:} Memory savings decrease as hidden dimension increases. This occurs because FlashAttention eliminates the $O(L^2)$ attention matrix but both methods must store Q, K, V matrices which scale as $O(L \cdot d)$. As $d$ grows, the Q/K/V memory dominates the attention matrix memory. At hidden=8192, the Q/K/V matrices (192MB) exceed the attention matrix (128MB for $L=2048$), eliminating FlashAttention's advantage.

This confirms that FlashAttention is most beneficial for \textit{long sequences with moderate hidden sizes}---exactly the regime modern LLMs operate in (e.g., 128K context with 4096-8192 hidden size).

\subsection{Triton Kernel Performance}

We implemented a simplified FlashAttention kernel using Triton, NVIDIA's GPU programming language. This demonstrates the performance potential when the algorithm is properly fused into a single GPU kernel.

\begin{table}[H]
\centering
\caption{Execution Time Comparison: Standard vs PyTorch FlashAttention vs Triton (RTX 3070)}
\label{tab:triton}
\begin{tabular}{@{}rrrrrr@{}}
\toprule
Seq Len & Standard (ms) & PyTorch FA (ms) & Triton FA (ms) & PyTorch Slowdown & Triton Speedup \\
\midrule
512 & 0.45 & 9.86 & 0.17 & 22.1x slower & \textbf{2.6x faster} \\
1024 & 1.07 & 7.00 & 0.38 & 6.5x slower & \textbf{2.8x faster} \\
2048 & 3.21 & 13.10 & 0.85 & 4.1x slower & \textbf{3.8x faster} \\
4096 & 11.42 & 40.93 & 3.06 & 3.6x slower & \textbf{3.7x faster} \\
8192 & 46.46 & 142.86 & 11.42 & 3.1x slower & \textbf{4.1x faster} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings:}
\begin{enumerate}
    \item \textbf{Triton achieves real speedup:} At 8192 tokens, Triton FlashAttention is 4.1x faster than standard attention while using 94\% less memory.

    \item \textbf{PyTorch implementation is slow:} The pure PyTorch version is 3-22x slower due to Python loop overhead and intermediate tensor allocations.

    \item \textbf{Kernel fusion is essential:} The Triton kernel fuses all operations (matmul, softmax, rescaling) into a single GPU kernel, eliminating memory bandwidth bottlenecks.

    \item \textbf{Numerical accuracy maintained:} Triton results differ by $< 2 \times 10^{-3}$ from standard attention, acceptable for ML applications.
\end{enumerate}

This demonstrates the full potential of FlashAttention: \textbf{memory efficiency AND speed gains} when properly implemented.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{plots/triton_comparison.png}
\caption{Execution time comparison across all implementations. The Triton kernel dramatically outperforms both standard attention and the pure PyTorch FlashAttention implementation, achieving the expected speedups from kernel fusion.}
\label{fig:triton_comparison}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{plots/triton_speedup.png}
\caption{Triton speedup factor vs standard attention increases with sequence length, reaching 4.1x at 8K tokens. This demonstrates how FlashAttention's advantages scale with longer sequences.}
\label{fig:triton_speedup}
\end{figure}

\subsection{CPU vs GPU Memory Measurements}

An important finding is the difference between CPU and GPU memory measurements for the same algorithm. We ran identical benchmarks on both platforms.

\begin{table}[H]
\centering
\caption{CPU vs GPU Memory Comparison: Same Algorithm, Different Overheads}
\label{tab:cpugpu}
\begin{tabular}{@{}rrrrrr@{}}
\toprule
Seq Len & CPU Flash (MB) & GPU Flash (MB) & CPU Savings & GPU Savings & Difference \\
\midrule
512 & 0.75 & 23.22 & 70.0\% & 20.3\% & 49.7\% \\
1024 & 1.51 & 38.31 & 83.2\% & 53.3\% & 29.9\% \\
2048 & 3.02 & 68.50 & 91.1\% & 75.9\% & 15.2\% \\
4096 & 6.03 & 128.88 & 95.4\% & 88.0\% & 7.4\% \\
8192 & 12.06 & 249.62 & 97.7\% & 94.0\% & 3.7\% \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Observations:}
\begin{enumerate}
    \item \textbf{GPU overhead is significant for small sequences:} At 512 tokens, CPU measures 0.75MB but GPU uses 23.22MB due to CUDA context, kernel launch buffers, and memory alignment.

    \item \textbf{Overhead becomes proportionally smaller:} The difference in savings percentages decreases from 49.7\% (512 tokens) to just 3.7\% (8192 tokens) as the actual computation dominates overhead.

    \item \textbf{Real-world performance differs from theoretical:} This highlights the importance of actual GPU benchmarking rather than relying on theoretical memory calculations.

    \item \textbf{Triton kernel validates the fused approach:} Our Triton implementation demonstrates that proper kernel fusion eliminates the Python loop overhead entirely, as shown in Table~\ref{tab:pytorch_vs_triton}.
\end{enumerate}

\begin{table}[H]
\centering
\caption{PyTorch GPU vs Triton GPU: Overhead Elimination Through Kernel Fusion}
\label{tab:pytorch_vs_triton}
\begin{tabular}{@{}rrrrr@{}}
\toprule
Seq Len & PyTorch FA (ms) & Triton FA (ms) & Overhead Reduction & vs Standard \\
\midrule
512 & 9.86 & 0.17 & 58.0x faster & 2.6x faster \\
1024 & 7.00 & 0.38 & 18.4x faster & 2.8x faster \\
2048 & 13.10 & 0.85 & 15.4x faster & 3.8x faster \\
4096 & 40.93 & 3.06 & 13.4x faster & 3.7x faster \\
8192 & 142.86 & 11.42 & 12.5x faster & 4.1x faster \\
\bottomrule
\end{tabular}
\end{table}

This table quantifies the massive overhead introduced by Python loops in the PyTorch implementation. The Triton kernel eliminates this overhead, achieving 12-58x speedup over PyTorch FlashAttention while maintaining the same memory efficiency. This confirms that the algorithmic memory savings (94\% at 8K tokens) combine with execution speed gains when intermediate tensor allocations are eliminated through kernel fusion.

This analysis demonstrates the full progression of FlashAttention optimizations:
\begin{itemize}
    \item \textbf{PyTorch CPU:} Proves algorithmic correctness and memory savings (97.7\% at 8K)
    \item \textbf{PyTorch GPU:} Shows real-world memory overhead (94\% savings at 8K, but slower execution)
    \item \textbf{Triton GPU:} Achieves both memory efficiency AND speed gains (4.1x faster at 8K tokens)
\end{itemize}

The Triton results validate that FlashAttention's theoretical advantages are fully realized when properly implemented with fused GPU kernels, making it practical for production deployment.

\section{Discussion}

\subsection{Key Findings}

\begin{enumerate}
    \item \textbf{Memory efficiency scales with sequence length:} The advantage of FlashAttention becomes more pronounced as sequences grow longer. At 8K tokens, we achieve 94.0\% memory savings (4184MB $\rightarrow$ 250MB).

    \item \textbf{Numerical stability is maintained:} Despite the incremental computation and rescaling, results remain numerically equivalent to standard attention (max diff $< 10^{-6}$).

    \item \textbf{The algorithm is the key insight:} Even without CUDA optimizations, the tiling approach fundamentally changes memory complexity from $O(L^2)$ to $O(L \cdot B)$ where $B$ is block size.

    \item \textbf{FlashAttention-2 provides algorithmic improvements:} Reduced FLOPs and better numerical properties make v2 superior to v1.
\end{enumerate}

\subsection{Theoretical Analysis}

The memory complexity comparison:
\begin{itemize}
    \item \textbf{Standard Attention:} $O(L^2)$ for attention matrix
    \item \textbf{FlashAttention:} $O(L \cdot B)$ for block of scores, where $B \ll L$
\end{itemize}

For $L = 16384$ and $B = 128$:
\begin{align}
    \text{Standard: } & 16384^2 = 268 \text{ million elements} \\
    \text{FlashAttention: } & 16384 \times 128 = 2.1 \text{ million elements}
\end{align}

This is a \textbf{128x reduction} in peak memory usage.

\subsection{Limitations}

\begin{enumerate}
    \item \textbf{No actual CUDA kernel:} Our implementation doesn't leverage GPU SRAM directly
    \item \textbf{Python overhead:} Loop-based implementation is slower than vectorized standard attention
    \item \textbf{No backward pass:} We only implement forward pass; training requires gradients
    \item \textbf{Block size tuning:} Optimal block size is hardware-dependent
\end{enumerate}

\subsection{Real-World Impact}

FlashAttention has enabled:
\begin{itemize}
    \item GPT-4's 128K token context window
    \item 2-4x faster training of LLMs
    \item Deployment of larger models on consumer hardware
    \item Reduced energy consumption in data centers
\end{itemize}

\section{Related Work}

\textbf{Sparse Attention:} Methods like Longformer \cite{beltagy2020longformer} and BigBird \cite{zaheer2020bigbird} reduce complexity by attending to only a subset of tokens. Unlike FlashAttention, these are \textit{approximate} methods.

\textbf{Linear Attention:} Performers \cite{choromanski2020rethinking} and Linear Transformers use kernel approximations for $O(L)$ complexity, but may sacrifice model quality.

\textbf{Memory-Efficient Backprop:} Gradient checkpointing reduces memory by recomputing activations. FlashAttention is orthogonal and can be combined with these methods.

\section{Conclusion}

This project implements an educational version of the FlashAttention algorithm with multiple approaches, showing:
\begin{itemize}
    \item Up to \textbf{94\% memory savings} compared to standard attention (4184MB $\rightarrow$ 250MB at 8K tokens)
    \item Up to \textbf{4.1x speedup} with Triton kernel (46ms $\rightarrow$ 11ms at 8K tokens)
    \item \textbf{Numerical equivalence} with standard attention (max diff $< 10^{-6}$ for PyTorch, $< 2 \times 10^{-3}$ for Triton)
    \item Implementations of \textbf{FlashAttention-1, FlashAttention-2, and Triton kernel} variants
    \item Support for \textbf{causal masking} for autoregressive models
    \item Comprehensive \textbf{test suite} with 23 passing tests
    \item Real GPU benchmarks on \textbf{NVIDIA RTX 3070} exploring both memory and speed characteristics
    \item Analysis of \textbf{CPU vs GPU overhead} explaining real-world performance differences
\end{itemize}

This implementation serves as an educational resource for understanding the core algorithm behind memory-efficient attention. The theoretical memory advantages are demonstrated through benchmarks, and the code provides a foundation for understanding how FlashAttention achieves its efficiency.

The quadratic memory bottleneck in attention is a fundamental challenge for scaling Transformers. FlashAttention provides an elegant solution that maintains exact computation while dramatically reducing memory requirements, enabling the long context windows that power modern AI assistants.

\section{Future Work}

\begin{enumerate}
    \item \textbf{CUDA Kernel Implementation:} Write fused CUDA kernels to achieve actual speedup
    \item \textbf{Backward Pass:} Implement gradient computation for training
    \item \textbf{FlashAttention-3:} Explore Hopper-specific optimizations
    \item \textbf{Multi-Query Attention:} Extend to grouped-query attention variants
    \item \textbf{Quantization:} Combine with INT8/FP16 quantization for further memory savings
\end{enumerate}

\bibliographystyle{plain}
\begin{thebibliography}{9}

\bibitem{vaswani2017attention}
Vaswani, A., et al. (2017).
\textit{Attention is all you need.}
Advances in Neural Information Processing Systems, 30.

\bibitem{dao2022flashattention}
Dao, T., et al. (2022).
\textit{FlashAttention: Fast and memory-efficient exact attention with IO-awareness.}
Advances in Neural Information Processing Systems, 35.

\bibitem{dao2023flashattention2}
Dao, T. (2023).
\textit{FlashAttention-2: Faster attention with better parallelism and work partitioning.}
arXiv preprint arXiv:2307.08691.

\bibitem{milakov2018online}
Milakov, M., \& Gimelshein, N. (2018).
\textit{Online normalizer calculation for softmax.}
arXiv preprint arXiv:1805.02867.

\bibitem{beltagy2020longformer}
Beltagy, I., Peters, M. E., \& Cohan, A. (2020).
\textit{Longformer: The long-document transformer.}
arXiv preprint arXiv:2004.05150.

\bibitem{zaheer2020bigbird}
Zaheer, M., et al. (2020).
\textit{Big bird: Transformers for longer sequences.}
Advances in Neural Information Processing Systems, 33.

\bibitem{choromanski2020rethinking}
Choromanski, K., et al. (2020).
\textit{Rethinking attention with performers.}
arXiv preprint arXiv:2009.14794.

\end{thebibliography}

\appendix

\section{Code Repository}
All code is available in the project repository:
\begin{itemize}
    \item \texttt{src/attention/standard\_attention.py} - Baseline implementation
    \item \texttt{src/utils/online\_softmax.py} - Online softmax algorithm
    \item \texttt{src/attention/flash\_attention\_single.py} - Single-row FlashAttention
    \item \texttt{src/attention/flash\_attention\_tiled.py} - Full tiled implementation (PyTorch)
    \item \texttt{src/attention/flash\_attention\_v2.py} - FlashAttention-2 variant with causal masking
    \item \texttt{src/attention/flash\_attention\_triton.py} - Triton GPU kernel implementation
    \item \texttt{tests/test\_correctness.py} - Comprehensive test suite (23 tests)
    \item \texttt{benchmarks/benchmark\_cpu.py} - CPU benchmarking code
    \item \texttt{benchmarks/benchmark\_gpu.py} - GPU benchmarking code
    \item \texttt{scripts/visualize\_results.py} - Plot generation
    \item \texttt{results/benchmark\_results.json} - Actual GPU benchmark data
\end{itemize}

\section{Reproducibility}
To reproduce our results:
\begin{lstlisting}[language=bash]
# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install torch matplotlib
# Note: For CUDA support, use:
# pip install torch --index-url https://download.pytorch.org/whl/cu121

# Run tests
python tests/test_correctness.py

# Run benchmarks
python benchmarks/benchmark_gpu.py

# Generate plots
python scripts/visualize_results.py
\end{lstlisting}

\end{document}
