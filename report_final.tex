\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{float}
\linespread{0.95}
\setlength{\intextsep}{6pt}
\setlength{\textfloatsep}{10pt}
\setlength{\floatsep}{8pt}
\usepackage{enumitem}
\setlist{nosep}
\usepackage{titling}
\setlength{\droptitle}{-2cm}  % adjust this number


\title{Memory-Efficient Attention Mechanisms:\\Implementation and Analysis of FlashAttention}
\author{Shuang Ma, Pablo Rodriguez, Pei Yu Lin\\
Department of Computer Science\\
University of California, Davis}
\date{November 2025}

\begin{document}

\maketitle

\begin{abstract}
Transformer models suffer from quadratic memory complexity in their self-attention mechanism, limiting their ability to process long sequences. We implement and analyze FlashAttention, a memory-efficient algorithm that reduces memory usage from $O(L^2)$ to $O(L)$ through tiling and online softmax computation. Our implementations include FlashAttention-1, FlashAttention-2, and a Triton GPU kernel, achieving up to 94\% memory savings and 4.1x speedup on an NVIDIA RTX 3070 GPU while maintaining numerical equivalence to standard attention. We benchmark across sequence lengths from 512 to 8K tokens and demonstrate both theoretical memory advantages and practical performance characteristics.
\end{abstract}

\section{Introduction}

Large Language Models rely on the Transformer architecture's self-attention mechanism, which requires $O(L^2)$ memory to store the attention matrix for sequences of length $L$. This quadratic scaling creates significant challenges: a 32K token sequence requires storing a 32K $\times$ 32K matrix (4GB for float32), limiting context windows on consumer GPUs and increasing training costs.

FlashAttention \cite{dao2022flashattention} addresses this bottleneck by processing attention in blocks, keeping intermediate results in fast on-chip memory (SRAM) rather than slow high-bandwidth memory (HBM). The algorithm maintains exact attention computation while fundamentally changing memory complexity from $O(L^2)$ to $O(L \cdot B)$ where $B$ is the block size.

\textbf{Our Contributions:}
\begin{itemize}
    \item Implementations of FlashAttention-1, FlashAttention-2, and Triton GPU kernel
    \item Demonstration of up to 94\% memory savings compared to standard attention
    \item 4.1x speedup with fused GPU kernels at 8K sequence length
    \item Comprehensive benchmarks analyzing memory, speed, and numerical accuracy
    \item Support for causal masking and multi-head attention
\end{itemize}

\section{Background and Method}

\subsection{Standard Self-Attention}
Self-attention computes: $\text{Output} = \text{softmax}(QK^T/\sqrt{d}) V$ where $Q, K, V \in \mathbb{R}^{L \times d}$. The standard implementation materializes the full $L \times L$ attention matrix, consuming $O(L^2)$ memory.

\subsection{Online Softmax Algorithm}
The key insight enabling FlashAttention is the online softmax algorithm \cite{milakov2018online}, which computes softmax incrementally using recurrence relations:
\begin{align}
    m_i &= \max(m_{i-1}, x_i) \\
    d_i' &= d_{i-1}' \cdot \exp(m_{i-1} - m_i) + \exp(x_i - m_i)
\end{align}
When the maximum changes, previous partial results are rescaled by $\exp(m_{\text{old}} - m_{\text{new}})$ to maintain correctness.

\subsection{FlashAttention Algorithm}
For each query row, FlashAttention processes K and V in blocks:

\begin{algorithm}[H]
\caption{FlashAttention-1 (Tiled)}
\begin{algorithmic}[1]
\STATE Initialize: $m \leftarrow -\infty$, $d \leftarrow 0$, $o \leftarrow \mathbf{0}$
\FOR{each block $B$ of K, V}
    \STATE $\mathbf{x} \leftarrow Q_{\text{row}} \cdot K_B^T / \sqrt{d}$
    \STATE $m_{\text{old}} \leftarrow m$, $m \leftarrow \max(m, \max(\mathbf{x}))$
    \STATE $\mathbf{e} \leftarrow \exp(\mathbf{x} - m)$
    \STATE $d \leftarrow d \cdot \exp(m_{\text{old}} - m) + \sum \mathbf{e}$
    \STATE $o \leftarrow o \cdot \exp(m_{\text{old}} - m) + \sum \mathbf{e} \odot V_B$
\ENDFOR
\RETURN $o / d$
\end{algorithmic}
\end{algorithm}

FlashAttention-2 \cite{dao2023flashattention2} improves this by maintaining unnormalized outputs and performing a single division at the end, reducing FLOPs by 31\% in our measurements.

\subsection{Implementation Details}
Our implementations include:
\begin{itemize}
    \item \textbf{PyTorch CPU/GPU:} Implementations demonstrating algorithmic memory savings
    \item \textbf{Triton kernel:} Fused GPU implementation achieving real speedups
    \item \textbf{Causal masking:} Support for autoregressive models
    \item \textbf{Multi-head attention:} Full batch and multi-head support
    \item \textbf{Comprehensive testing:} 23 unit tests validating numerical correctness
\end{itemize}

\section{Experiments}

\subsection{Experimental Setup}
\textbf{Hardware:} NVIDIA RTX 3070 (8GB VRAM), Intel CPU. \textbf{Software:} PyTorch 2.9.1, CUDA 12.1, Python 3.10. \textbf{Configuration:} Batch size 1, 8 attention heads, head dimension 64, float32 precision.

\subsection{Memory Efficiency Results}

\begin{table}[H]
\centering
\caption{Memory Comparison: Standard vs FlashAttention (RTX 3070)}
\label{tab:memory}
\begin{tabular}{@{}rrrrrr@{}}
\toprule
Seq Len & Std Mem (MB) & Flash Mem (MB) & Savings & Max Diff \\
\midrule
512 & 29.12 & 23.22 & 20.3\% & $4.2 \times 10^{-7}$ \\
1024 & 82.12 & 38.31 & 53.3\% & $3.6 \times 10^{-7}$ \\
2048 & 284.12 & 68.50 & 75.9\% & $5.4 \times 10^{-7}$ \\
4096 & 1072.12 & 128.88 & 88.0\% & $3.9 \times 10^{-7}$ \\
8192 & 4184.12 & 249.62 & 94.0\% & $4.1 \times 10^{-7}$ \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{plots_h200/memory_scaling.png}
\caption{Memory use scaling: standard attention grows as $O(L^2)$, FlashAttention as $O(L)$.}

\label{fig:memory}
\end{figure}

Memory savings increase with sequence length, reaching 94\% at 8K tokens (4184MB $\rightarrow$ 250MB). All implementations maintain numerical equivalence with max difference $< 10^{-6}$.

\subsection{Speed Performance with Triton Kernel}

\begin{table}[H]
\centering
\caption{Execution Time: Standard vs Triton FlashAttention (RTX 3070)}
\label{tab:triton}
\begin{tabular}{@{}rrrrr@{}}
\toprule
Seq Len & Standard (ms) & Triton FA (ms) & Speedup \\
\midrule
512 & 0.45 & 0.17 & 2.6x \\
1024 & 1.07 & 0.38 & 2.8x \\
2048 & 3.21 & 0.85 & 3.8x \\
4096 & 11.42 & 3.06 & 3.7x \\
8192 & 46.46 & 11.42 & \textbf{4.1x} \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.75\textwidth]{plots/triton_comparison.png}
\caption{Execution time comparison for standard attention and the Triton kernel.}

\label{fig:triton}
\end{figure}

The Triton kernel demonstrates FlashAttention's full potential: combining 94\% memory savings with 4.1x speedup at 8K tokens. Pure PyTorch implementations prove algorithmic correctness but suffer from Python loop overhead.

\subsection{Algorithm Comparison}

FlashAttention-2's optimization of maintaining unnormalized outputs provides measurable improvements:

\begin{table}[H]
\centering
\caption{FlashAttention-1 vs FlashAttention-2 (seq\_len=512, batch=2, heads=4)}
\label{tab:v1v2}
\begin{tabular}{@{}lrrr@{}}
\toprule
Metric & FA-1 & FA-2 & Improvement \\
\midrule
Execution Time (ms) & 7.02 & 5.35 & 1.31x faster \\
Max Diff vs Standard & $2.98 \times 10^{-7}$ & $2.68 \times 10^{-7}$ & Better accuracy \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Hidden Dimension Analysis}

\begin{table}[H]
\centering
\caption{Memory Savings vs Hidden Dimension (seq\_len=2048, 8 heads)}
\label{tab:hidden}
\begin{tabular}{@{}rrrrr@{}}
\toprule
Hidden Size & Std Mem (MB) & Flash Mem (MB) & Savings \\
\midrule
512 & 284.12 & 52.50 & 81.5\% \\
2048 & 344.12 & 160.50 & 53.4\% \\
8192 & 584.12 & 592.50 & -1.4\% \\
\bottomrule
\end{tabular}
\end{table}

Memory savings decrease as hidden dimension increases because both methods must store Q, K, V matrices ($O(L \cdot d)$). As $d$ grows, Q/K/V memory dominates attention matrix memory. This confirms FlashAttention is most beneficial for \textit{long sequences with moderate hidden sizes}---the regime modern LLMs operate in.


\section{Discussion}

FlashAttention lowers memory use for long sequences and keeps accuracy close to standard attention. Gains increase at large $L$. The Triton version gives strong speed boosts thanks to fused computations.

\section{Work Distribution}

\begin{itemize}
    \item \textbf{Pei Yu Lin:} Worked on FlashAttention-2 and the Triton version of FlashAttention-2. Helped refine the GPU kernels, checked speed and memory results, and helped prepare parts of the presentation.
    \item \textbf{Shuang Ma:} Worked on early FlashAttention code, helped set up testing, and ran tests on the H200 GPU. Also assisted in checking algorithm steps and experiment setup.
    \item \textbf{Pablo Rodriguez:} Wrote the first PyTorch versions of FlashAttention, built the Triton kernel for FlashAttention-1, ran the main GPU tests, and created the visualizations for the report.
\end{itemize}


\section{Conclusion}

We implemented and analyzed FlashAttention, demonstrating up to 94\% memory savings and 4.1x speedup while maintaining exact attention computation. Our work includes three implementations (FlashAttention-1, FlashAttention-2, Triton kernel), comprehensive benchmarks, and analysis of practical performance characteristics. The implementations serve as educational resources for understanding memory-efficient attention algorithms that enable modern LLMs to process long context windows.

Future work includes implementing the backward pass for training, exploring FlashAttention-3 optimizations, and combining with quantization for further memory savings.

\bibliographystyle{plain}
\begin{thebibliography}{9}

\bibitem{vaswani2017attention}
Vaswani, A., et al. (2017).
\textit{Attention is all you need.}
NeurIPS.

\bibitem{dao2022flashattention}
Dao, T., et al. (2022).
\textit{FlashAttention: Fast and memory-efficient exact attention with IO-awareness.}
NeurIPS.

\bibitem{dao2023flashattention2}
Dao, T. (2023).
\textit{FlashAttention-2: Faster attention with better parallelism and work partitioning.}
arXiv preprint arXiv:2307.08691.

\bibitem{milakov2018online}
Milakov, M., \& Gimelshein, N. (2018).
\textit{Online normalizer calculation for softmax.}
arXiv preprint arXiv:1805.02867.

\bibitem{beltagy2020longformer}
Beltagy, I., Peters, M. E., \& Cohan, A. (2020).
\textit{Longformer: The long-document transformer.}
arXiv preprint arXiv:2004.05150.

\bibitem{zaheer2020bigbird}
Zaheer, M., et al. (2020).
\textit{Big bird: Transformers for longer sequences.}
NeurIPS.

\bibitem{choromanski2020rethinking}
Choromanski, K., et al. (2020).
\textit{Rethinking attention with performers.}
arXiv preprint arXiv:2009.14794.

\end{thebibliography}

\end{document}
